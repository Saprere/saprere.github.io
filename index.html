<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lucas Represa - Personal Projects</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header>
        <h1>Unreal-HRI</h1>
        <p>Lucas Represa</p>
    </header>

    <nav id="table-of-contents">
        <h2>Table of Contents</h2>
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#methodology">Methodology</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
        </ul>
    </nav>

    <main>
        <section class="project">
            <h2>Optimizing Emotion Recognition in Social Robots: The Role of Physical Characteristics</h2>

            <h3>Abstract</h3>
            <p>This study explores emotion recognition challenges in social robotics, focusing on how robot characteristics and human attributes affect estimation accuracy. It introduces a novel methodology using 3D technologies to generate synthetic datasets simulating diverse human-robot interactions. The research investigates the impact of robot physical properties (yaw angles, distances, sizes) on emotion estimation and examines potential biases from human attributes (age, gender, race). By analyzing these factors' interplay, the study aims to enhance emotion recognition systems' reliability and fairness, improve benchmarks, and guide future training efforts, ultimately contributing to the development of more intuitive and responsive social robots.</p>

            <h3>Project Origin</h3>
            <p>This project was born from the observation that traditional computer vision face algorithms, such as Haar Cascade, often fail when applied to small robots. The unique perspectives and limitations of these robots create challenges in accurately detecting and interpreting human facial expressions.</p>
            
            <div class="image-container">
                <img src="images/metahuman_fusion.png" alt="MetaHuman Fusion">
                <img src="images/robot_sizes.png" alt="Robot Sizes">
            </div>
            <p>The images above illustrate the concept: on the left, we see the diversity of human faces created using MetaHuman technology, and on the right, the various sizes of robots considered in this study.</p>

            <h3>The MetaHumanAfterAll Dataset</h3>
            <p>To address the limitations of existing datasets and better understand the challenges faced by robots of different sizes, we created the "MetaHumanAfterAll" dataset. This synthetic dataset comprises approximately 14,000 images of diverse virtual humans displaying seven basic emotions. Key features include:</p>
            <ul>
                <li>66 Synthetic Humans with diverse attributes</li>
                <li>Three angles: -45°, 0°, and 45°</li>
                <li>Three distances: 50 cm, 100 cm, and 200 cm</li>
                <li>Four robot sizes: babySize (42.0 cm), dogSize (89.0 cm), childSize (120.0 cm), and adultSize (170.0 cm)</li>
            </ul>
            <p>This dataset provides a comprehensive resource for training and evaluating emotion recognition models in human-robot interaction scenarios.</p>

            <figure>
                <img src="images/metahumanafterall.png" alt="MetaHumanAfterAll Dataset Sample">
                <figcaption>Figure: Sample images from the MetaHumanAfterAll dataset, showcasing diverse synthetic humans and emotions from different robot viewpoints.</figcaption>
            </figure>

            <h3>Study Overview</h3>
            <p>Using the MetaHumanAfterAll dataset, we investigated how robot physical characteristics affect the quality of social cue estimation in human-robot interactions. We analyzed the effects of robot size, angle, and distance on emotion detection accuracy.</p>
            
            <h3>Key Findings:</h3>
            <ul>
                <li>Larger robots (adultSize category) consistently outperform smaller ones in emotion detection accuracy.</li>
                <li>Optimal configurations for emotion detection include moderate distances (200 cm) and frontal angles (0 degrees).</li>
                <li>Complex interactions exist between robot size, distance, and angle, affecting model performance.</li>
            </ul>

            <h3>Visualizations:</h3>
            <figure>
                <img src="plots/confidence_box_plot_robotsize.png" alt="Confidence Box Plot of Chosen Emotion Scores per Robot Size">
                <figcaption>Figure 1: Confidence Box Plot of Chosen Emotion Scores per Robot Size</figcaption>
            </figure>

            <figure>
                <img src="plots/1d_heatmap_accuracy_robot.png" alt="One-Dimensional Heatmap of Mean Accuracies Across Robot Characteristics Groups">
                <figcaption>Figure 2: One-Dimensional Heatmap of Mean Accuracies Across Robot Characteristics Groups</figcaption>
            </figure>

            <figure>
                <img src="plots/2d_heatmap_accuracy_robot_characteristics.png" alt="Two-Dimensional Heatmap of Mean Accuracies of Robot Characteristics Groups">
                <figcaption>Figure 3: Two-Dimensional Heatmap of Mean Accuracies of Robot Characteristics Groups</figcaption>
            </figure>

            <p>This study provides valuable insights for the design and deployment of social robots, emphasizing the need to consider physical characteristics in optimizing emotion detection capabilities.</p>
        </section>

        <!-- Add more project sections as needed -->
    </main>

    <footer>
        <p>&copy; 2024 Lucas Represa. All rights reserved.</p>
        <p>Project conducted under the supervision of Pr. Salzmann <a href="https://www.epfl.ch/labs/cvlab/" target="_blank">(EPFL, CVLAB)</a> and Dr. Thomas Janssoone <a href="https://enchanted.tools" target="_blank">(Enchanted Tools)</a>.</p>
    </footer>

    <button id="back-to-top" title="Back to Top">↑</button>
    <script src="js/script.js"></script>
</body>
</html>
